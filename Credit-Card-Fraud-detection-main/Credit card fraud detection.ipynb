{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f87c64-f244-4705-9dd9-ef78de8bfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CreditCardFraudDetection\").getOrCreate()\n",
    "\n",
    "# Test the Spark session\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330ec48-481e-4eb2-9c0c-f9eb20380ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/hingn/Videos/CFD/creditcard.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87ead7-43ad-4452-b2b8-9844338f4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf492301-6ca2-4853-b834-f859fc824c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "\n",
    "# Counting both null and NaN values for each column\n",
    "null_counts = [\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "\n",
    "# Display the count of null and NaN values for each column\n",
    "df.select(null_counts).show()\n",
    "\n",
    "# Optional: Calculate the total count of null and NaN values across all columns\n",
    "total_nulls = sum(\n",
    "    when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)\n",
    "    for c in df.columns\n",
    ")\n",
    "df.select(total_nulls.alias(\"Total_Null_or_NaN_Values\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb77915-56f2-4d24-96d2-23c0dda9f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Class').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddbfb0-42c1-41d4-9f1d-c535f73a8f46",
   "metadata": {},
   "source": [
    " #### Over and down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa1eaf-9135-4840-8ef7-8e128908f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_df = df.filter(col(\"Class\") == 1)\n",
    "nofr_df = df.filter(col(\"Class\") == 0)\n",
    "ratio = int(nofr_df.count()/fr_df.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcfe3a-30c6-4ce2-bef1-87d3d475612b",
   "metadata": {},
   "source": [
    "#### The non fraudulent to fraudulent ratio is 577 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e7d68-c14b-4763-9daf-04f7b66e33e5",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ab65a-b63e-4f1b-9f07-00d6581a12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, array, lit\n",
    "\n",
    "# Duplicate the minority rows\n",
    "oversampled_df = fr_df.withColumn(\"dummy\", explode(array([lit(x) for x in range(ratio)]))).drop('dummy')\n",
    "\n",
    "# Combine both oversampled minority rows and previous majority rows\n",
    "df_o = nofr_df.union(oversampled_df)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "df_o.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479fcfb-7295-4a5d-841d-e1b81044af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_o.count(), len(df_o.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117cdd1-0736-4815-b135-1a3f2ca386b5",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f340708-7a40-469d-9b05-72d90c87a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the majority class\n",
    "sampled_majority_df = nofr_df.sample(False, 1/ratio)\n",
    "\n",
    "# Combine the sampled majority class with the minority class\n",
    "df_u = sampled_majority_df.union(fr_df)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf92980-1576-41fd-ba22-cbea72d10953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_u.count(), len(df_u.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32868f8-6066-49c5-9579-894995def836",
   "metadata": {},
   "source": [
    "#### Learning Machines Spark's machine learning techniques require feature values to be in a vector. As a result, we will utilize VectorAssembler to convert all feature values to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb40c7c-449a-41d4-830b-e3f6622da157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8f677-6ad6-45b1-b68c-1ceeff2e0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "cols.remove('Time')\n",
    "cols.remove('Class')\n",
    "\n",
    "# We specify the object from the VectorAssembler class.\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "\n",
    "# Now we transform the data into vectors\n",
    "data_o = assembler.transform(df_o)\n",
    "\n",
    "data_o.select('features', 'Class').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe2dd4-6ea4-4124-ad18-2e44e70e1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_o = data_o.select('features', 'Class')\n",
    "data_o.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17b9b1-da4f-48eb-a83d-b62fc59d919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o, test_o = data_o.randomSplit([0.7,0.3])\n",
    "train_o.show(5)\n",
    "test_o.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2df91-7602-4b43-8f78-4ca5ace0d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do the same prep for undersampled dataframe. But in one box with no displays.\n",
    "\n",
    "# Transform the data into vectors\n",
    "data_u = assembler.transform(df_u)\n",
    "\n",
    "data_u = data_u.select('features', 'Class')\n",
    "train_u, test_u = data_u.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b50ce-faa2-4124-9eb4-78c1bb6c1454",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "Before experimenting with various machine learning algorithms, we'd like to scale the data. Although the StandardScaler method might be employed, the Naive Bayes algorithm requires that no feature values be negative. As a result, we will scale the values between 0 and 1 using the MinMaxScaler function. We will use the same scaled dataset in all methods to make a more realistic comparison.\n",
    "\n",
    "Finally, three different machine learning approaches will be used to compare the oversampled and undersampled datasets.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3163a5-dcc2-4f1e-a0c1-b268cea9146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')\n",
    "data_o = minmax_scaler.fit(data_o).transform(data_o)\n",
    "data_u = minmax_scaler.fit(data_u).transform(data_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a4682-940a-4493-9f03-6eecfb878f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_o, test_o = data_o.randomSplit([0.7,0.3])\n",
    "train_u, test_u = data_u.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860f78b-f4e2-4f48-84b3-f2434c8aa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_o.show(5))\n",
    "print(train_u.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b68421-b684-438a-867d-8016f63da39c",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Logistic Regression is a widely used statistical method for predicting binary outcomes. Its simplicity and interpretability make it a popular choice for binary classification problems. In this project, we have employed Logistic Regression to predict outcomes in a dataset characterized by class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f377c7-373d-4b63-85b8-bfffdb9cbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(labelCol='Class', featuresCol='scaled_features', maxIter=40)\n",
    "model_o = logReg.fit(train_o)\n",
    "model_u = logReg.fit(train_u)\n",
    "predicted_test_o = model_o.transform(test_o)\n",
    "predicted_test_u = model_u.transform(test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d842b8-7792-42b6-80d4-baf4cd735f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_o.select('Class', 'prediction').show(10)\n",
    "predicted_test_u.select('Class', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef03e2-73ed-4c57-9cc1-867a5daf4ab8",
   "metadata": {},
   "source": [
    "#### Results\n",
    "The performance of the Logistic Regression models was evaluated using several metrics, including accuracy, weighted precision, weighted recall, and the F1 score. These metrics provided insights into how well the models performed on both oversampled and undersampled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9086b-693e-40ac-88f0-1deaefe2a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_LR_o = evaluator.evaluate(predicted_test_o)\n",
    "accuracy_LR_u = evaluator.evaluate(predicted_test_u)\n",
    "evaluator.setMetricName(\"weightedPrecision\")\n",
    "weightedPrecision_LR_o = evaluator.evaluate(predicted_test_o)\n",
    "weightedPrecision_LR_u = evaluator.evaluate(predicted_test_u)\n",
    "evaluator.setMetricName(\"weightedRecall\")\n",
    "weightedRecall_LR_o = evaluator.evaluate(predicted_test_o)\n",
    "weightedRecall_LR_u = evaluator.evaluate(predicted_test_u)\n",
    "print(f'Logistic Regression - Oversampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_LR_o:.4f}\\n'\n",
    "      f'  Weighted Precision:{weightedPrecision_LR_o:.4f}\\n'\n",
    "      f'  Weighted Recall:   {weightedRecall_LR_o:.4f}\\n')\n",
    "\n",
    "print(f'Logistic Regression - Undersampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_LR_u:.4f}\\n'\n",
    "      f'  Weighted Precision:{weightedPrecision_LR_u:.4f}\\n'\n",
    "      f'  Weighted Recall:   {weightedRecall_LR_u:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984596c-ed3f-4b05-ad92-baa3d7dc27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming you have Logistic Regression model predictions: predicted_test_o and predicted_test_u\n",
    "\n",
    "# Initialize the evaluator for the F1 metric\n",
    "evaluator_LR_f1 = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Evaluate F1 Score for the oversampled dataset\n",
    "f1_score_LR_o = evaluator_LR_f1.evaluate(predicted_test_o)\n",
    "\n",
    "# Evaluate F1 Score for the undersampled dataset\n",
    "f1_score_LR_u = evaluator_LR_f1.evaluate(predicted_test_u)\n",
    "\n",
    "# Print results\n",
    "print('Logistic Regression - F1 Score Oversampled:', f1_score_LR_o)\n",
    "print('Logistic Regression - F1 Score Undersampled:', f1_score_LR_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bba88-461f-48cf-bcb2-7dad529a50b2",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n",
    "Random Forest is an ensemble learning method known for its high accuracy, ability to run on large datasets, and capability to handle numerous input variables without variable deletion. It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2b06f-dd84-403d-93dd-528cd3d009f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(labelCol='Class', featuresCol='scaled_features', numTrees=40)\n",
    "model_o = random_forest_classifier.fit(train_o)\n",
    "model_u = random_forest_classifier.fit(train_u)\n",
    "\n",
    "\n",
    "predicted_test_rf_o = model_o.transform(test_o)\n",
    "predicted_test_rf_u = model_u.transform(test_u)\n",
    "\n",
    "predicted_test_rf_o.select('Class', 'prediction').show(10)\n",
    "predicted_test_rf_u.select('Class', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5c134-cd7b-4c58-8c4b-f77f24bfefe8",
   "metadata": {},
   "source": [
    "#### Results and Discussion\n",
    "The performance evaluation revealed how the Random Forest Classifier responded to the class imbalance problem in different scenarios. By comparing its performance on the oversampled versus the undersampled dataset, we were able to gauge the impact of these techniques on the classifier's ability to generalize and make accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabd49f-7a74-434f-ae5a-9b7dfcfaf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_rf = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_rf_o = evaluator_rf.evaluate(predicted_test_rf_o)\n",
    "accuracy_rf_u = evaluator_rf.evaluate(predicted_test_rf_u)\n",
    "evaluator_rf.setMetricName(\"weightedPrecision\")\n",
    "weightedPrecision_rf_o = evaluator_rf.evaluate(predicted_test_rf_o)\n",
    "weightedPrecision_rf_u = evaluator_rf.evaluate(predicted_test_rf_u)\n",
    "evaluator_rf.setMetricName(\"weightedRecall\")\n",
    "weightedRecall_rf_o = evaluator_rf.evaluate(predicted_test_rf_o)\n",
    "weightedRecall_rf_u = evaluator_rf.evaluate(predicted_test_rf_u)\n",
    "print(f'Random Forest - Oversampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_rf_o:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_rf_o:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_rf_o:.4f}\\n')\n",
    "\n",
    "print(f'Random Forest - Undersampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_rf_u:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_rf_u:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_rf_u:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352aca94-ebdf-42f5-81f1-550b7a74eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Initialize the evaluator for the F1 metric\n",
    "evaluator_rf_f1 = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Evaluate F1 Score for the oversampled dataset\n",
    "f1_score_rf_o = evaluator_rf_f1.evaluate(predicted_test_rf_o)\n",
    "\n",
    "# Evaluate F1 Score for the undersampled dataset\n",
    "f1_score_rf_u = evaluator_rf_f1.evaluate(predicted_test_rf_u)\n",
    "\n",
    "# Print results\n",
    "print('Random Forest - F1 Score Oversampled:', f1_score_rf_o)\n",
    "print('Random Forest - F1 Score Undersampled:', f1_score_rf_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc85740-41f9-447f-bece-a27a20095941",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes is a simple yet effective probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features. It is particularly known for its simplicity, efficiency, and good performance in a wide range of problem settings, including text classification and medical diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd354e62-f1dd-4cbc-bc97-0b98588c4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes(featuresCol='scaled_features', labelCol='Class', smoothing=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b85a6-07ff-4e9a-9513-1d4a73086128",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_o = naive_bayes.fit(train_o)\n",
    "model_u = naive_bayes.fit(train_u)\n",
    "predicted_test_nb_o = model_o.transform(test_o)\n",
    "predicted_test_nb_u = model_u.transform(test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f44de71-8505-4f43-aaf0-f41f0626301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_nb_o.select('Class', 'prediction').show(10)\n",
    "predicted_test_nb_u.select('Class', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e17e8-44b5-4270-b581-d8565e608014",
   "metadata": {},
   "source": [
    "#### Results and Discussion\n",
    "We examined the effectiveness of the Naive Bayes classifier in handling class imbalance by evaluating its performance on the oversampled and undersampled datasets. The results were analyzed through various metrics, providing insights into the model's ability to generalize and accurately predict outcomes in both scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023e96b-2ab8-46d9-b4d8-96ede4a3f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_nb = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_NB_o = evaluator_nb.evaluate(predicted_test_nb_o)\n",
    "accuracy_NB_u = evaluator_nb.evaluate(predicted_test_nb_u)\n",
    "evaluator_nb.setMetricName(\"weightedPrecision\")\n",
    "weightedPrecision_NB_o = evaluator_nb.evaluate(predicted_test_nb_o)\n",
    "weightedPrecision_NB_u = evaluator_nb.evaluate(predicted_test_nb_u)\n",
    "\n",
    "evaluator_nb.setMetricName(\"weightedRecall\")\n",
    "weightedRecall_NB_o = evaluator_nb.evaluate(predicted_test_nb_o)\n",
    "weightedRecall_NB_u = evaluator_nb.evaluate(predicted_test_nb_u)\n",
    "\n",
    "print(f'Naive Bayes - Oversampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_NB_o:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_NB_o:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_NB_o:.4f}\\n')\n",
    "\n",
    "print(f'Naive Bayes - Undersampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_NB_u:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_NB_u:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_NB_u:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cd499-7b9c-4a33-9060-1830299b337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Existing code for fitting the model and predictions\n",
    "naive_bayes = NaiveBayes(featuresCol='scaled_features', labelCol='Class', smoothing=1.0)\n",
    "model_o = naive_bayes.fit(train_o)\n",
    "model_u = naive_bayes.fit(train_u)\n",
    "predicted_test_nb_o = model_o.transform(test_o)\n",
    "predicted_test_nb_u = model_u.transform(test_u)\n",
    "\n",
    "# Use MulticlassClassificationEvaluator with 'f1' metric\n",
    "evaluator_nb_f1 = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Evaluate F1 score\n",
    "f1_score_NB_o = evaluator_nb_f1.evaluate(predicted_test_nb_o)\n",
    "f1_score_NB_u = evaluator_nb_f1.evaluate(predicted_test_nb_u)\n",
    "\n",
    "# Print results\n",
    "print('F1 Score Oversampled =', f1_score_NB_o)\n",
    "print('F1 Score Undersampled =', f1_score_NB_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ff835-9b06-45bd-9e6a-592e1a9dc100",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier\n",
    "Gradient Boosting is an ensemble learning technique that builds and combines multiple weak models (typically decision trees) to create a robust predictive model. It's particularly known for its effectiveness in handling various types of data, including imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242817a3-6218-44fc-b431-ee75d24f2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_class = GBTClassifier(labelCol='Class', featuresCol='scaled_features')\n",
    "model_o = gradient_boost_class.fit(train_o)\n",
    "model_u = gradient_boost_class.fit(train_u)\n",
    "\n",
    "predicted_test_gbc_o = model_o.transform(test_o)\n",
    "predicted_test_gbc_u = model_u.transform(test_u)\n",
    "\n",
    "predicted_test_gbc_o.select('Class', 'prediction').show(10)\n",
    "predicted_test_gbc_u.select('Class', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e9460-0490-4901-8a8c-efa35779d1ed",
   "metadata": {},
   "source": [
    "#### Results and Discussion\n",
    "The evaluation metrics provided insights into the efficacy of the Gradient Boosting Classifier under different class distributions. The analysis focused on:\n",
    "\n",
    "1. The accuracy of the model in correctly predicting outcomes.\n",
    "2. The precision and recall, giving us an understanding of the model's ability to identify the minority class correctly.\n",
    "3. The F1 score, which combines precision and recall into a single metric, offering a balanced view of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22e6a2-0808-44b0-a44f-825b69d1a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_gbc = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_gbc_o = evaluator_gbc.evaluate(predicted_test_gbc_o)\n",
    "accuracy_gbc_u = evaluator_gbc.evaluate(predicted_test_gbc_u)\n",
    "evaluator_gbc.setMetricName(\"weightedPrecision\")\n",
    "weightedPrecision_gbc_o = evaluator_gbc.evaluate(predicted_test_gbc_o)\n",
    "weightedPrecision_gbc_u = evaluator_gbc.evaluate(predicted_test_gbc_u)\n",
    "evaluator_gbc.setMetricName(\"weightedRecall\")\n",
    "weightedRecall_gbc_o = evaluator_gbc.evaluate(predicted_test_gbc_o)\n",
    "weightedRecall_gbc_u = evaluator_gbc.evaluate(predicted_test_gbc_u)\n",
    "\n",
    "print(f'Gradient Boosted Classifier - Oversampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_gbc_o:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_gbc_o:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_gbc_o:.4f}\\n')\n",
    "\n",
    "print(f'Gradient Boosted Classifier - Undersampled Data:\\n'\n",
    "      f'  Accuracy:          {accuracy_gbc_u:.4f}\\n'\n",
    "      f'  Weighted Precision: {weightedPrecision_gbc_u:.4f}\\n'\n",
    "      f'  Weighted Recall:    {weightedRecall_gbc_u:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561c70f-0f2c-4735-8296-0523eeda2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the evaluator for the F1 metric\n",
    "evaluator_gbc_f1 = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Evaluate F1 Score for the oversampled dataset\n",
    "f1_score_gbc_o = evaluator_gbc_f1.evaluate(predicted_test_gbc_o)\n",
    "\n",
    "# Evaluate F1 Score for the undersampled dataset\n",
    "f1_score_gbc_u = evaluator_gbc_f1.evaluate(predicted_test_gbc_u)\n",
    "\n",
    "# Print results\n",
    "print('Gradient Boosted Classifier - F1 Score Oversampled:', f1_score_gbc_o)\n",
    "print('Gradient Boosted Classifier - F1 Score Undersampled:', f1_score_gbc_u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673ca5a-d029-4491-9f9c-2ca205d4a050",
   "metadata": {},
   "source": [
    "#### Dataset Overview for Fraud Detection\n",
    "This PySpark script efficiently analyzes a financial transaction dataset, summarizing key aspects crucial for fraud detection. It computes the total number of transactions, features, and the proportion of normal versus fraudulent activities. These insights are vital for understanding dataset characteristics, especially the class imbalance critical in modeling fraud detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c80cd-e81b-4e4f-9190-3603c5015273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Total number of transactions\n",
    "total_transactions = df.count()\n",
    "\n",
    "# Total number of columns\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "# Total number of features (assuming the last column is the label)\n",
    "total_features = total_columns - 1\n",
    "\n",
    "# Total number of label(s)\n",
    "total_labels = 1\n",
    "\n",
    "# Total number of normal transactions\n",
    "total_normal_transactions = df.filter(col('Class') == 0).count()\n",
    "\n",
    "# Total number of fraudulent transactions\n",
    "total_fraudulent_transactions = df.filter(col('Class') == 1).count()\n",
    "\n",
    "# Percentage of fraudulent transactions\n",
    "percentage_fraudulent = (total_fraudulent_transactions / total_transactions) * 100\n",
    "\n",
    "# Percentage of normal transactions\n",
    "percentage_normal = (total_normal_transactions / total_transactions) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of transactions:\", total_transactions)\n",
    "print(\"Total number of columns:\", total_columns)\n",
    "print(\"Total number of features:\", total_features)\n",
    "print(\"Total number of label(s):\", total_labels)\n",
    "print(\"Total number of normal transactions:\", total_normal_transactions)\n",
    "print(\"Total number of fraudulent transactions:\", total_fraudulent_transactions)\n",
    "print(\"Percentage of fraudulent transactions: {:.4f}%\".format(percentage_fraudulent))\n",
    "print(\"Percentage of normal transactions: {:.4f}%\".format(percentage_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99311e52-666e-4c81-92c8-8d942cb4316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Headers for the table\n",
    "headers = [\"Model\", \"Data Type\", \"Accuracy\", \"Weighted Precision\", \"Weighted Recall\", \"F1 Score\"]\n",
    "\n",
    "# Data for the table\n",
    "data = [\n",
    "    [\"Naive Bayes\", \"Oversampled\", accuracy_NB_o, weightedPrecision_NB_o, weightedRecall_NB_o, f1_score_NB_o],\n",
    "    [\"Naive Bayes\", \"Undersampled\", accuracy_NB_u, weightedPrecision_NB_u, weightedRecall_NB_u, f1_score_NB_u],\n",
    "    [\"Gradient Boosted\", \"Oversampled\", accuracy_gbc_o, weightedPrecision_gbc_o, weightedRecall_gbc_o, f1_score_gbc_o],\n",
    "    [\"Gradient Boosted\", \"Undersampled\", accuracy_gbc_u, weightedPrecision_gbc_u, weightedRecall_gbc_u, f1_score_gbc_u],\n",
    "    [\"Random Forest\", \"Oversampled\", accuracy_rf_o, weightedPrecision_rf_o, weightedRecall_rf_o, f1_score_rf_o],\n",
    "    [\"Random Forest\", \"Undersampled\", accuracy_rf_u, weightedPrecision_rf_u, weightedRecall_rf_u, f1_score_rf_u],\n",
    "    [\"Logistic Regression\", \"Oversampled\", accuracy_LR_o, weightedPrecision_LR_o, weightedRecall_LR_o, f1_score_LR_o],\n",
    "    [\"Logistic Regression\", \"Undersampled\", accuracy_LR_u, weightedPrecision_LR_u, weightedRecall_LR_u, f1_score_LR_u]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f8510-5783-496a-8b62-f385af8babc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "models = ['Naive Bayes', 'Gradient Boosted', 'Random Forest', 'Logistic Regression']\n",
    "accuracy_oversampled = [accuracy_NB_o, accuracy_gbc_o, accuracy_rf_o, accuracy_LR_o]\n",
    "precision_oversampled = [weightedPrecision_NB_o, weightedPrecision_gbc_o, weightedPrecision_rf_o, weightedPrecision_LR_o]\n",
    "recall_oversampled = [weightedRecall_NB_o, weightedRecall_gbc_o, weightedRecall_rf_o, weightedRecall_LR_o]\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(len(models)))\n",
    "width = 0.25\n",
    "\n",
    "# Plotting the bars\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Create a bar with accuracy data,\n",
    "# in position pos,\n",
    "plt.bar(pos,\n",
    "        accuracy_oversampled,\n",
    "        width,\n",
    "        alpha=0.5,\n",
    "        color='#EE3224',\n",
    "        label=models[0])\n",
    "\n",
    "# Create a bar with precision data,\n",
    "# in position pos + some width buffer,\n",
    "plt.bar([p + width for p in pos],\n",
    "        precision_oversampled,\n",
    "        width,\n",
    "        alpha=0.5,\n",
    "        color='#F78F1E',\n",
    "        label=models[1])\n",
    "\n",
    "# Create a bar with recall data,\n",
    "# in position pos + width buffer,\n",
    "plt.bar([p + width*2 for p in pos],\n",
    "        recall_oversampled,\n",
    "        width,\n",
    "        alpha=0.5,\n",
    "        color='#FFC222',\n",
    "        label=models[2])\n",
    "\n",
    "# Set the y axis label\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "# Set the chart's title\n",
    "ax.set_title('Model Performance (Oversampled Data)')\n",
    "\n",
    "# Set the position of the x ticks\n",
    "ax.set_xticks([p + 1.5 * width for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "ax.set_xticklabels(models)\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Accuracy', 'Precision', 'Recall'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f86be4-66f9-41d1-863d-e783a6cd20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example model names\n",
    "models = ['Naive Bayes', 'Gradient Boosted', 'Random Forest', 'Logistic Regression']\n",
    "\n",
    "# Example F1 scores for each model (replace with your actual scores)\n",
    "f1_scores_oversampled = [f1_score_NB_o, f1_score_gbc_o, f1_score_rf_o, f1_score_LR_o] # Replace with your F1 scores for oversampled\n",
    "f1_scores_undersampled = [f1_score_NB_u, f1_score_gbc_u, f1_score_rf_u, f1_score_LR_u] # Replace with your F1 scores for undersampled\n",
    "\n",
    "# Setting the positions for the bars\n",
    "pos = list(range(len(models)))\n",
    "width = 0.35  # Width of a bar\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.bar([p - width/2 for p in pos],\n",
    "        f1_scores_oversampled,\n",
    "        width,\n",
    "        alpha=0.5,\n",
    "        color='#EE3224',\n",
    "        label='Oversampled')\n",
    "\n",
    "plt.bar([p + width/2 for p in pos],\n",
    "        f1_scores_undersampled,\n",
    "        width,\n",
    "        alpha=0.5,\n",
    "        color='#F78F1E',\n",
    "        label='Undersampled')\n",
    "\n",
    "# Setting axis labels, title, and ticks\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score by Model and Sampling Method')\n",
    "ax.set_xticks(pos)\n",
    "ax.set_xticklabels(models)\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Oversampled', 'Undersampled'], loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034a4e1-3fb8-4ae1-8248-5c6cfd972d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
